{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run util.ipynb\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this implementation only works when class labels are integer values 0,1,2,...n because\n",
    "# the class labels are used as indices in a numpy array\n",
    "\n",
    "# Below is the general and naive classifier\n",
    "# smoothing is used to prevent multiplying by 0\n",
    "\n",
    "class BayesClassifier(object):\n",
    "    \n",
    "    def __init__(self, naive = 1):\n",
    "        self.naive = naive\n",
    "    \n",
    "    def fit(self, X, Y, smoothing=10e-3):\n",
    "        \n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Create dictionaries to store the gaussian parameters and priors\n",
    "        self.gaussians = {}\n",
    "        self.priors = {}\n",
    "        \n",
    "        # Obtain the possible output values of Y\n",
    "        labels = set(Y)\n",
    "        \n",
    "        # Iterate through all class labels (outcomes) and find the parameters for each\n",
    "        for c in labels:\n",
    "            \n",
    "            # grabs all data in x for each possible outcome in Y\n",
    "            current_x = X[Y == c]\n",
    "            \n",
    "            # find mean and variance for each possible outcome data\n",
    "            if self.naive:\n",
    "                self.gaussians[c] = {\n",
    "                'mean': current_x.mean(axis=0), # takes the mean of all N data points within each feature\n",
    "                'var': current_x.var(axis=0) + smoothing #likewise as above with variance\n",
    "                #the final result is an array of feature means (taken for all data points matching an outcome)\n",
    "                }\n",
    "            else:\n",
    "                self.gaussians[c] = {\n",
    "                'mean': current_x.mean(axis=0), # takes the mean of all N data points within each feature\n",
    "                'var': np.cov(current_x.T) + np.eye(D) * smoothing\n",
    "                    # np.cov estimates a covariance matrix, must transpose to be along dimension D instead of N\n",
    "                    # np.eye creates a matrix of ones along the diagonal with D rows\n",
    "                }\n",
    "            # calculate the prior distribution associated with it\n",
    "            self.priors[c] = float(len(Y[Y==c])) / len(Y)\n",
    "            \n",
    "            \n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # grab the dimensions of the data being predicted\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # The number of possible outcomes we've stored gaussian parameters for\n",
    "        K = len(self.gaussians)\n",
    "        \n",
    "        # Stores the posteriors for each possible outcome. size (data requiring prediction, # of outcomes)\n",
    "        P = np.zeros((N, K))\n",
    "        \n",
    "        # Iterate through gaussian parameters (for each outcome) and calculate posterior\n",
    "        for c, g in self.gaussians.items():\n",
    "            mean, var = g['mean'], g['var'] # returns means (data matching class label) for each feature\n",
    "            P[:, c] = mvn.logpdf(X, mean=mean, cov=var) + np.log(self.priors[c])\n",
    "            # computes the log probability for each data point (across all features) given mean and covariance matrix\n",
    "            # so basically tells us P(Data given Class) * P(Class)\n",
    "            \n",
    "        # Pick the most likely of possible outcomes for each data point (compares probability of each class given the data)\n",
    "        return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9988\n",
      "Test accuracy: 0.9392\n"
     ]
    }
   ],
   "source": [
    "# Testing with the MNIST data set\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X,Y = get_data(10000)\n",
    "    Ntrain = int(len(Y) / 2)\n",
    "    Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "    Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "    \n",
    "    model = BayesClassifier(naive = False)\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "    print(\"Test accuracy:\", model.score(Xtest, Ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
