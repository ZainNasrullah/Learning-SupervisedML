{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run util.ipynb\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is a numpy array\n",
    "def entropy(y):\n",
    "    \n",
    "    # find all cases where y is equal to 1\n",
    "    N = len(y)\n",
    "    s1 = (y==1).sum()\n",
    "    \n",
    "    # cases where there is only 1 class means there is no entropy\n",
    "    if s1 == 0 or N == s1:\n",
    "        return 0 \n",
    "    \n",
    "    # Probability of y == 1\n",
    "    p1 = float(s1) / N \n",
    "    p0 = 1 - p1\n",
    "    \n",
    "    # return the entropy \n",
    "    return -p0 * np.log2(p0) - p1 * np.log2(p1)\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    \n",
    "    def __init__(self, depth=0, max_depth=None):\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        # base case where the training data only has 1 data point or one class label\n",
    "        if len(Y) == 1 or len(set(Y)) == 1:\n",
    "            self.col = None\n",
    "            self.split = None\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.prediction = Y[0] # return whatever that class label is\n",
    "            \n",
    "        else:\n",
    "            # Grab number of features\n",
    "            D = X.shape[1]\n",
    "            cols = range(D)\n",
    "            \n",
    "            # Need to find and store the best place to split\n",
    "            max_ig = 0\n",
    "            best_col = None\n",
    "            best_split = None\n",
    "            \n",
    "            # Iterate through columns in the training data\n",
    "            # find best split based on maximizing information gain\n",
    "            for col in cols:\n",
    "                \n",
    "                # return the best split location and what the ig is\n",
    "                ig, split = self.find_split(X, Y, col)\n",
    "                \n",
    "                # use found split is better than current max split\n",
    "                if ig > max_ig:\n",
    "                    max_ig = ig\n",
    "                    best_col = col\n",
    "                    best_split = split\n",
    "            \n",
    "            # base case where information gain is 0\n",
    "            # take the most likely class label\n",
    "            if max_ig == 0:\n",
    "                self.col = None\n",
    "                self.split = None\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.prediction = np.round(Y.mean())\n",
    "            \n",
    "            # when information gain is not zero\n",
    "            else:\n",
    "                self.col = best_col\n",
    "                self.split = best_split\n",
    "                \n",
    "                # case where we've hit the maximum specified depth\n",
    "                # returns 2 predictions, one for the left side and one for the right side\n",
    "                if self.depth == self.max_depth:\n",
    "                    self.left = None\n",
    "                    self.right = None\n",
    "                    self.prediction = [\n",
    "                        np.round(Y[X[:,best_col] < self.split].mean()),\n",
    "                        np.round(Y[X[:, best_col] >= self.split].mean())\n",
    "                    ]\n",
    "                # use recursion otherwise to continue with the decision tree\n",
    "                else:\n",
    "                    # gives us the index where X is less than the best split\n",
    "                    left_idx = (X[:, best_col] < best_split)\n",
    "                    Xleft = X[left_idx]\n",
    "                    Yleft = Y[left_idx]\n",
    "                    self.left = TreeNode(self.depth + 1, self.max_depth)\n",
    "                    self.left.fit(Xleft, Yleft)\n",
    "                    \n",
    "                    # Index where X is more than the best split\n",
    "                    right_idx = (X[:, best_col] >= best_split)\n",
    "                    Xright = X[right_idx]\n",
    "                    Yright = Y[right_idx]\n",
    "                    self.right = TreeNode(self.depth + 1, self.max_depth)\n",
    "                    self.right.fit(Xright, Yright)\n",
    "                \n",
    "    def find_split(self, X, Y, col):\n",
    "        x_values = X[:, col]\n",
    "        sort_idx = np.argsort(x_values) # returns indices of sorted array\n",
    "        x_values = x_values[sort_idx] # sorts the values\n",
    "        y_values = Y[sort_idx] # sorted y values in exact same way\n",
    "        \n",
    "        # shifts y such that that compares each value against the next\n",
    "        # the nonzero function returns the indices where a comparison is not the same\n",
    "        boundaries = np.nonzero(y_values[:-1] != y_values[1:])\n",
    "        # in summary, all it does it tell us where the boundaries are\n",
    "        \n",
    "        best_split = None\n",
    "        max_ig = 0\n",
    "\n",
    "        # iterate through boundaries and find the one that maxes ig\n",
    "        for i in np.nditer(boundaries):\n",
    "            # take midpoint of boundaries\n",
    "            split = (x_values[i] + x_values[i+1]) / 2\n",
    "\n",
    "            # calculate ig given x and y values\n",
    "            ig = self.information_gain(x_values, y_values, split)\n",
    "            if ig > max_ig:\n",
    "                max_ig = ig\n",
    "                best_split = split\n",
    "\n",
    "        return max_ig, best_split\n",
    "\n",
    "\n",
    "    def information_gain(self, x, y, split):\n",
    "        # divide data based on the split\n",
    "        \n",
    "        y0 = y[x < split]\n",
    "        y1 = y[x >= split]\n",
    "\n",
    "        # calculate length of the data\n",
    "        N = len(y)\n",
    "        y0len = len(y0)\n",
    "\n",
    "        # if only one class then no information gain\n",
    "        if y0len == 0 or y0len == N:\n",
    "            return 0\n",
    "\n",
    "        # calculate the proportions\n",
    "        p0 = float(len(y0)) / N\n",
    "        p1 = 1 - p0\n",
    "\n",
    "        # return the information gain\n",
    "        return entropy(y) - p0*entropy(y0) - p1*entropy(y1)\n",
    "\n",
    "\n",
    "    # This function predicts the class label for a particular data point\n",
    "    def predict_one(self,x):\n",
    "\n",
    "        # If a split ocurred during the fitting\n",
    "        if self.col is not None and self.split is not None:\n",
    "            \n",
    "            # select column with highest information gain\n",
    "            feature = x[self.col]\n",
    "\n",
    "            # goes to left side\n",
    "            if feature < self.split: \n",
    "                if self.left: # is there a left child?\n",
    "                    p = self.left.predict_one(x) # recursive call on child\n",
    "                else:\n",
    "                    p = self.prediction[0] # return left prediction\n",
    "\n",
    "            # go to the right\n",
    "            else: \n",
    "                if self.right: # is there a left child?\n",
    "                    p = self.right.predict_one(x) # recursive call on child\n",
    "                else:\n",
    "                    p = self.prediction[1] # return right prediction\n",
    "\n",
    "        # There were no splits\n",
    "        else:\n",
    "            p = self.prediction\n",
    "\n",
    "        return p\n",
    "\n",
    "    # This function predicts the class label for all data points by repeatedly calling predict_one\n",
    "    def predict(self, X):\n",
    "        N = len(X)\n",
    "        P = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            P[i] = self.predict_one(X[i])\n",
    "        return P\n",
    "\n",
    "# Wrapper class for tree node, not entirely sure why this is needed other than abstraction\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth = None):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.root = TreeNode(max_depth=self.max_depth)\n",
    "        self.root.fit(X,Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.root.predict(X)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# self written from pseudocode\n",
    "\n",
    "# calculating entropy\n",
    "def entropy(y):\n",
    "    N = len(y)\n",
    "    y0 = len(y[y==0])\n",
    "    \n",
    "    if y0 == 0 or y0 == N:\n",
    "        return 0\n",
    "    \n",
    "    # float is critical here\n",
    "    p0 = float(y0) / N\n",
    "    p1 = 1 - p0\n",
    "    return -p0*np.log2(p0) - p1*np.log2(p1)\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    \n",
    "    # set current and max depth\n",
    "    def __init__(self, depth = 0, maxdepth = None):\n",
    "        self.depth = depth\n",
    "        self.maxdepth = maxdepth\n",
    "    \n",
    "    # finding possible splits in a column and which one maximizes information gain\n",
    "    def find_split(self, X, Y, col):\n",
    "        # sorts x and correspondingly y. Determines where y changes from one class label to another. \n",
    "        x = X[:, col]\n",
    "        x_sorted_indices = np.argsort(x)\n",
    "        x_sorted = x[x_sorted_indices]\n",
    "        y_sorted = Y[x_sorted_indices]\n",
    "        boundaries = np.nonzero(y_sorted[:-1] != y_sorted[1:])\n",
    "\n",
    "        max_ig = 0\n",
    "        best_split = None\n",
    "        \n",
    "        # find which boundary maximizes information gain\n",
    "        for i in np.nditer(boundaries):\n",
    "            split = (x[i] + x[i + 1]) / 2\n",
    "            \n",
    "            ig = self.information_gain(x_sorted, y_sorted, split)\n",
    "            if ig > max_ig:\n",
    "                max_ig = ig\n",
    "                best_split = split\n",
    "        \n",
    "        return max_ig, best_split\n",
    "            \n",
    "    def information_gain(self, X, Y, split):\n",
    "        # find proportion of Y less than the split and greater than the split\n",
    "        \n",
    "        y0 = Y[X < split]\n",
    "        y1 = Y[X >= split]\n",
    "        y0len = len(y0)\n",
    "        N = len(Y)\n",
    "        \n",
    "        if y0len == 0 or y0len == N:\n",
    "            return 0\n",
    "        \n",
    "        p0 = float(len(y0)) / N\n",
    "        p1 = 1 - p0\n",
    "    \n",
    "        # calculate information gain\n",
    "        return entropy(Y) - p0*entropy(y0) - p1*entropy(y1)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        # if length of Y equals 1 or there is only one class left, return that class as the prediction\n",
    "        if len(Y) == 1 or len(set(Y)) == 1:\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.split = None\n",
    "            self.col = None\n",
    "            self.prediction = Y[0]\n",
    "            \n",
    "        # ordinary case\n",
    "        else:\n",
    "            D = X.shape[1]\n",
    "\n",
    "            max_ig = 0\n",
    "            best_col = None\n",
    "            best_split = None\n",
    "            \n",
    "            # iterate through columns and find the one that maximizes information gain\n",
    "            for col in range(D):\n",
    "\n",
    "                ig, split = self.find_split(X, Y, col)\n",
    "                if ig > max_ig:\n",
    "                    best_col = col\n",
    "                    max_ig = ig\n",
    "                    best_split = split\n",
    "            \n",
    "            # if information gain is zero, then just take the most likely class label\n",
    "            if max_ig == 0:\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.split = None\n",
    "                self.col = None\n",
    "                self.prediction = np.round(Y.mean())\n",
    "            \n",
    "            # ordinary case\n",
    "            else:\n",
    "                self.col = best_col\n",
    "                self.split = best_split\n",
    "                \n",
    "                # if at max depth, describe left and right predictions as most likely outcomes of the split data\n",
    "                if self.depth == self.maxdepth:\n",
    "                    self.left = None\n",
    "                    self.right = None\n",
    "                    self.prediction = [\n",
    "                        np.round(Y[X[:, best_col] < self.split].mean()), \n",
    "                        np.round(Y[X[:, best_col] >= self.split].mean())\n",
    "                    ]\n",
    "                \n",
    "                # otherwise, create left and right children using the split and recurse\n",
    "                else:\n",
    "                    left_idx = (X[:, best_col] < best_split)\n",
    "                    Xleft = X[left_idx]\n",
    "                    Yleft = Y[left_idx]\n",
    "                    self.left = DecisionTreeNode(self.depth + 1, self.maxdepth)\n",
    "                    self.left.fit(Xleft, Yleft)\n",
    "\n",
    "                    right_idx = (X[:, best_col] >= best_split)\n",
    "                    Xright = X[right_idx]\n",
    "                    Yright = Y[right_idx]\n",
    "                    self.right = DecisionTreeNode(self.depth+1, self.maxdepth)\n",
    "                    self.right.fit(Xright,Yright)\n",
    "    \n",
    "    # loop over rows and predict each one at a time\n",
    "    def predict(self, X):\n",
    "        N = len(X)\n",
    "        P = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            P[i] = self.predict_one(X[i])\n",
    "        return P\n",
    "    \n",
    "    # if there is split, recurse to the next level of the decision tree if possible; otherwise, take prediction\n",
    "    # if no split, just take the prediction\n",
    "    def predict_one(self, x):\n",
    "        \n",
    "        if self.col != None and self.split != None:\n",
    "            feature = x[self.col]\n",
    "            \n",
    "            #go left\n",
    "            if feature < self.split:\n",
    "                if self.left:\n",
    "                    p = self.left.predict_one(x)\n",
    "                else:\n",
    "                    p = self.prediction[0]\n",
    "            \n",
    "            #go right\n",
    "            else:\n",
    "                if self.right:\n",
    "                    p = self.right.predict_one(x)\n",
    "                else:\n",
    "                    p = self.prediction[1]\n",
    "        else:\n",
    "            p = self.prediction\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    X, Y = get_data()\n",
    "    \n",
    "    # Classifying whether number is 0 or 1\n",
    "    idx = np.logical_or(Y == 0, Y ==1) # returns true where Y = 0 or 1\n",
    "    # keeps indexes where idx is true\n",
    "    X = X[idx] \n",
    "    Y = Y[idx]\n",
    "    \n",
    "    Ntrain = int(len(Y) / 2)\n",
    "    Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "    Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "    \n",
    "    model = DecisionTree()\n",
    "    model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.993194192377\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     4384\n",
       "False      24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = model.predict(Xtest)\n",
    "pd.Series(P==Ytest).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DecisionTreeNode()\n",
    "model2.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.993421052632\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model2.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model2.score(Xtest, Ytest))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
